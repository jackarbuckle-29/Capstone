---
title: "PostTune_Code_032124"
author: "Kaylin Slattery"
date: "2024-03-21"
output: html_document
---

```{r packages warnings=FALSE, error=FALSE, message=FALSE}
library(tidyverse)
library(DescTools)
library(janitor)
library(ggplot2)
library(jpeg)
# library(chron)
# library(lubridate)
# library(anytime)
library(ggpubr)
# library(devtools) 
#install_github("AppliedDataSciencePartners/xgboostExplainer")
library(xgboost) # Load XGBoost
library(caret) # Load Caret
# library(xgboostExplainer) # Load XGboost Explainer
# library(pROC) # Load proc
library(SHAPforxgboost) # Load shap for XGBoost
library(plyr)
library(Metrics)
library(kableExtra)
```

```{r nd colors}
# setting ND colors 
nd_navy <- "#0C2340"
nd_green <- "#00843D"
nd_gold <- "#C99700"
```

```{r}
load(".\\CRC_RDAs\\comb_soc\\alpha_lambda.rda")
#res_db, bst_lambda, bst_alpha, g_5,
load(".\\CRC_RDAs\\comb_soc\\bst_final_mod.rda")
# bst_final_mod, bst_pred_data, tuned_importance, imp_plot, xgb_imp_df,
# full_preds_df, post_tune_mod_pred_viz,
# post_tune_mod_pred_viz_2,post_tune_diff_hist
load(".\\CRC_RDAs\\comb_soc\\eta.rda")
#bst_mod_1, bst_mod_2, bst_mod_3, bst_mod_4, bst_mod_5,g_6, g_7, plot_data,
load(".\\CRC_RDAs\\comb_soc\\gamma.rda")
# gam_df, bst_gamma,
load(".\\CRC_RDAs\\comb_soc\\min_child_max_depth.rda")
# res_db, bst_max_depth, bst_min_child, g_2, 
load(".\\CRC_RDAs\\comb_soc\\pre_tune_files.rda")
# prelim_importance, bst_split_pred_data_pre_tune, pred_viz_pre_tune, diff_hist_pre_tune
load(".\\CRC_RDAs\\comb_soc\\sample_colsample.rda")
# res_db, bst_col_samp, bst_sub_samp, g_4, 

bst_split_pred_data_pre_tune
bst_pred_data
full_pred_soc_df <- rbind.data.frame(bst_split_pred_data_pre_tune, bst_pred_data)
```

```{r}
# bst_final_mod, bst_pred_data, tuned_importance, imp_plot, xgb_imp_df,
# full_preds_df, post_tune_mod_pred_viz,
# post_tune_mod_pred_viz_2,post_tune_diff_hist

xgb_imp_df
ggplot(data=xgb_imp_df[1:10,], aes(x=Importance, 
y=reorder(Feature, -Importance, decreasing = TRUE))) + 
  theme_minimal() + geom_col(fill=nd_navy) +
  labs(title = "Soccer XGBoost Model Top 10 Most Important Variables", 
       x = "Importance", y = "Variable") 

#xgb_imp_df[1:10, ]$Feature


xgb_imp_df$z_var_bin <- ifelse(grepl("z", xgb_imp_df$Feature), "z-score", "non-z")

# Now plot using this new column for fill
ggplot(data=xgb_imp_df[1:10, ], aes(x=Importance,
                                    y=reorder(Feature, -Importance, decreasing = TRUE))) +
  theme_minimal() +
  geom_col(aes(fill=z_var_bin)) +  
  scale_fill_manual(values=c("z-score"=nd_green, "non-z"=nd_navy)) +
  labs(title="Top Predictors - Soccer XGBoost", x="Importance", y="Variable")
```

```{r}
bst_final_mod$evaluation_log %>% 
  pivot_longer(cols = c(train_rmse), names_to = "phase") %>% 
  ggplot(aes(x = iter, y = value, color = phase)) + geom_line(color=nd_navy, size=2) + theme_minimal() +
  labs(title = "Train RMSE - Soccer XGBoost")
```

## Histogram Pre & Post-Tune
```{r}
ggplot(full_pred_soc_df, aes(x = difference, y=..density.., fill = time)) + 
  geom_histogram(data = subset(full_pred_soc_df, time == "pre-tune"),
                 alpha = 0.5, position = "identity", binwidth =50) + 
  geom_histogram(data = subset(full_pred_soc_df, time == "post-tune"),
                 alpha = 0.5, position = "identity", binwidth = 50) + 
  scale_fill_manual(values = c("pre-tune" = nd_navy, "post-tune" = nd_green)) + 
  labs(title = "Overlaying Histograms - Soccer",
       x = "Residual (Actual - Predicted)", y = "Frequency") + 
  theme_minimal() + xlim(-1500, 1500)
```


```{r}
post <- full_pred_soc_df %>% filter(time == "post-tune")
dim(post[abs(post$difference) <= 150, ])[1]/(dim(post)[1]) #75.9%
```

# Prediction Instances
```{r}
full_preds_df
data <- read.csv(".\\Exported_CSVs\\data_cleaned.csv") 
#data <- read.csv(".\\Exported_CSVs\\data_cleaned.csv") 

data <- data %>%
  dplyr::rename(high_speed_distance=high_speed_running_distance_session)

not_include_indoor_vec <- c("duration", "distance", "meta", "total_effort", 
                            "heart_rate", "hr", "velocity", "acceleration",
                            "deceleration", "metre", "meterage",
                            "exertion_index","max_vel_max", 
                            "footstrikes", "running_series_count", 
                            "running_imbalance")

indoor_full_df <- data %>%
  filter(location=="indoor") %>%
  select(-contains(not_include_indoor_vec))

var_to_pred <- "high_speed_distance"

set.seed(33) #setting seed for reproducing same samples

#selecting only outdoor rows to predict high speed distance
outdoor_df <- data %>%
  filter(location == "outdoor")
sample_rows_2k <- sample(dim(outdoor_df)[1], 2000)

# creating a model subset for testing the model logic before 
# sub_model_df <- data[sample_rows_2k, ]  %>% 
#   select(all_of(c( "high_speed_distance", hypo_cols_vec)))

# mod_df <- data %>% 
#   select(all_of(c(var_to_pred, hypo_cols_vec)))
# 
# sub_model_df <- data %>% 
#   select(all_of(c("name_id", "date", var_to_pred, hypo_cols_vec, second_set_hypos)))

num_col_names <- data %>% 
  select_if(is.numeric) %>% 
  names()

num_col_names_indoor <- num_col_names[num_col_names %in% names(indoor_full_df)] 

to_remove_vec <- c("unix", "position_name")

sub_df <- data %>% 
  select(all_of(c("unique_session", "name_id", num_col_names_indoor))) %>%
  select(-contains(to_remove_vec))


############################# PLAYER Z-SCORES ############################# 
players <- unique(data$name_id)
z_prep_mod_df <- sub_df
full_df <- as.data.frame(matrix(ncol=dim(z_prep_mod_df)[2]))
names(full_df) <- names(z_prep_mod_df)

for(i in 1:length(players)){
  player_df <- z_prep_mod_df[z_prep_mod_df$name_id == players[i],]
  
  df_non_num <- player_df %>% select_if(is.character)
  df_num <- player_df %>% select_if(is.numeric)
  
  df_num <- scale(df_num)
  combined <- cbind(df_non_num, df_num)
  full_df <- rbind(full_df, combined)
}

full_df <- full_df[-1,]
names(full_df) <- paste0(names(full_df), "_z") 
z_score_df <- full_df %>% select(-contains(c("name_id")))

############################# MERGING Z AND REGULAR DATA ############################# 
comb_z_df <- merge(data, z_score_df, 
                   by.x = "unique_session", by.y = "unique_session_z",
                   all.x=TRUE)
#head(comb_z_df)

num_col_names_indoor <- num_col_names[num_col_names %in% names(indoor_full_df)]
to_remove_vec <- c("unix", "position_name")

test_df <- data %>%
  select(all_of(c("unique_session","name_id", "high_speed_distance",
                  num_col_names_indoor))) %>%
  select(-contains(to_remove_vec))

comb_z_df_outdoor <- comb_z_df %>% filter(high_speed_distance != 0)
low_lim <- quantile(comb_z_df_outdoor$high_speed_distance, .10)
up_lim <- quantile(comb_z_df_outdoor$high_speed_distance, .90)

include_mod_cols <- c("high_speed_distance", names(test_df), names(z_score_df))
comb_z_df_mod <- comb_z_df_outdoor[, names(comb_z_df_outdoor) %in% include_mod_cols]


############################# MODELING PREP ############################# 
model_df <- comb_z_df_mod %>% 
  select(!contains(c("unique_session", "name_id", "high_speed_distance_z", "team_gender_z", 
                     "activity_type_z", "date_z", "location_z"))) %>%
  filter(high_speed_distance >= low_lim) %>% #& total_high_speed_distance <= up_lim)  %>%
  select(all_of("high_speed_distance"), everything())

############################# PARTITIONING ############################# 
set.seed(33) # Set Seed
split_ratio <- 0.7
train_row_ind <- sample(x=nrow(model_df), 
                        size=floor(nrow(model_df) * split_ratio))

# Split the data into training and testing sets
train_data <- model_df[train_row_ind, ]
test_data <- model_df[-train_row_ind, ]

train_response = train_data[,1]
train_x = train_data[,-1]
test_response = test_data[,1]
test_x = test_data[,-1]

response <- model_df$high_speed_distance

# Create training data XGBOOST
dtrain <- xgb.DMatrix(data = as.matrix(train_data[ ,-1]), label = train_response)
# Create test data XGBOOST
dtest <- xgb.DMatrix(data = as.matrix(test_data[ ,-1]),label = test_response)
```
# OUTLIER COMPARISON
```{r}
cols_df <- test_data
post_tune_res <- full_preds_df %>% filter(time == "post-tune")


test_info_df <- merge(cols_df, post_tune_res, 
      by.x = "high_speed_distance", by.y = "actual",
      all.x=TRUE)

test_info_df %>% select()

top_vars_10 <- c(xgb_imp_df[1:10, ]$Feature)

col_avg_df <- test_info_df %>% 
  select(all_of(top_vars_10)) %>% 
  colMeans() %>% as.data.frame()

outlier_df <- test_info_df %>% 
  filter(abs(difference)>500) %>% 
  select(all_of(c("high_speed_distance", "difference", top_vars_10)))


col_avg_df

diff_avg_df <-
  
# pred vs actual in cross validation group
# correlations 
  
# final model, assign every sample a number 1:10, loop, 1 becomes test set first, 2-10, training data; predict for group 1 store those preds, continue through
# do that across all the models, see whicha agree
  
# elastic net
# lasso with an alpha between 0 and 1
```

